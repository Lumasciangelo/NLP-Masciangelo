{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Trabajo Práctico final Procesamiento del Lenguaje Natural**\n",
        "\n",
        "*Lucía Masciángelo*\n",
        "\n",
        "*Fecha de entrega:* 21/07/2024\n",
        "\n",
        "*Profesores:* Juan Pablo Manson, Alan Geary\n"
      ],
      "metadata": {
        "id": "3epC1f4Ch8qW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 1: RAG**"
      ],
      "metadata": {
        "id": "Iximork-hywB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparo el entorno"
      ],
      "metadata": {
        "id": "C70B7JzRcMws"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IS0Wte4cCa7"
      },
      "outputs": [],
      "source": [
        "!pip install docx2txt\n",
        "!pip install PyPDF2\n",
        "!pip install unidecode\n",
        "!pip install nltk\n",
        "!pip install langchain\n",
        "!pip install sentence-transformers\n",
        "!pip install txtai\n",
        "!pip install chromadb\n",
        "!pip install pandas\n",
        "!pip install SPARQLWrapper\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install requests\n",
        "!pip install python-decouple\n",
        "!pip install llm-templates"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conecto con Google Drive"
      ],
      "metadata": {
        "id": "5XNccTBrhYBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar acceso a google drive para poder acceder a los archivos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PqGmXs4dhlq",
        "outputId": "2c55097e-1035-4bf8-8b2d-5488ad6d334b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codigo completo de RAG"
      ],
      "metadata": {
        "id": "UTzEnJAFhcnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from txtai.embeddings import Embeddings\n",
        "import chromadb\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import requests\n",
        "from decouple import config\n",
        "import pprint\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Extraer texto de archivos\n",
        "def extraer_texto_docx(ruta_docx):\n",
        "    doc = Document(ruta_docx)\n",
        "    return \"\\n\".join([parrafo.text for parrafo in doc.paragraphs])\n",
        "\n",
        "def extraer_texto_pdf(ruta_pdf):\n",
        "    with open(ruta_pdf, 'rb') as archivo_pdf:\n",
        "        lector = PyPDF2.PdfReader(archivo_pdf)\n",
        "        return \"\".join([pagina.extract_text() for pagina in lector.pages])\n",
        "\n",
        "def extraer_texto_csv(ruta_csv):\n",
        "    df = pd.read_excel(ruta_csv)\n",
        "    return \"\\n\".join([\"\\t\".join(map(str, fila)) for fila in df.values])\n",
        "\n",
        "# Procesar texto\n",
        "def procesar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = unidecode(texto)\n",
        "    texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto)\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    word_tokens = word_tokenize(texto)\n",
        "    return \" \".join([word for word in word_tokens if word.casefold() not in stop_words])\n",
        "\n",
        "# Ruta de la carpeta en Google Drive\n",
        "carpeta_drive = '/content/drive/MyDrive/Datasets NLP/'\n",
        "\n",
        "# Cargar archivos\n",
        "ruta_docx = carpeta_drive + 'El cine de Argentina - Historia.docx'\n",
        "ruta_pdf = carpeta_drive + '11-dugini-informacion-historica-cine-argentino-rhaya.pdf'\n",
        "ruta_csv1 = carpeta_drive + 'Precio de las entradas a lo largo del tiempo.xlsx'\n",
        "ruta_csv2 = carpeta_drive + 'Relacion entrada-poblacion.xlsx'\n",
        "\n",
        "texto_docx = extraer_texto_docx(ruta_docx)\n",
        "texto_pdf = extraer_texto_pdf(ruta_pdf)\n",
        "texto_csv1 = extraer_texto_csv(ruta_csv1)\n",
        "texto_csv2 = extraer_texto_csv(ruta_csv2)\n",
        "\n",
        "texto_combinado = \"\\n\".join([texto_docx, texto_pdf, texto_csv1, texto_csv2])\n",
        "texto_procesado = procesar_texto(texto_combinado)\n",
        "\n",
        "# Crear embeddings\n",
        "model_name = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"\n",
        "embed_model = SentenceTransformer(model_name)\n",
        "chunks = [texto_procesado[i:i + 512] for i in range(0, len(texto_procesado), 512)]\n",
        "chunks_embeddings = embed_model.encode(chunks).tolist()\n",
        "\n",
        "# Crear y agregar embeddings a ChromaDB\n",
        "client = chromadb.Client()\n",
        "collection_name = \"cine_argentino\"\n",
        "# if collection_name in client.list_collections():\n",
        "collection = client.get_collection(collection_name)\n",
        "# else:\n",
        "#     collection = client.create_collection(collection_name)\n",
        "\n",
        "for i, (chunk, embedding) in enumerate(zip(chunks, chunks_embeddings)):\n",
        "    collection.add(ids=[str(i)], documents=[chunk], embeddings=[embedding])\n",
        "\n",
        "# Configurar y usar txtai\n",
        "embeddings = Embeddings({\"path\": model_name})\n",
        "embeddings.index(({\"id\": str(i), \"text\": chunk, \"embedding\": embedding} for i, (chunk, embedding) in enumerate(zip(chunks, chunks_embeddings))))\n",
        "\n",
        "# Funciones para generar respuestas\n",
        "HUGGINGFACE_TOKEN = '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'\n",
        "LLM_MODEL = {'model': 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'system_role': False, 'template': 'mistral'}\n",
        "\n",
        "import requests\n",
        "\n",
        "def test_token():\n",
        "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
        "    response = requests.get(\"https://api-inference.huggingface.co/models\", headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "print(test_token())\n",
        "\n",
        "def chat_template(messages, add_generation_prompt=True):\n",
        "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
        "    data = {\n",
        "        \"inputs\": messages,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 150,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_k\": 50,\n",
        "            \"top_p\": 0.95\n",
        "        }\n",
        "    }\n",
        "    response = requests.post(f\"https://api-inference.huggingface.co/models/{LLM_MODEL['model']}\", headers=headers, json=data)\n",
        "    return response.json()[0]['generated_text']\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Cargar el modelo y el tokenizador\n",
        "model_name = \"gpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "def generate_answer(prompt: str, max_new_tokens: int = 150):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=max_new_tokens, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Ejemplo de uso\n",
        "prompt = \"¿Cuál fue la primera película argentina?\"\n",
        "print(generate_answer(prompt))\n",
        "\n",
        "\n",
        "def prepare_prompt(query_str, context_str):\n",
        "    template = (\n",
        "        \"La información de contexto es la siguiente:\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"{context_str}\\n\"\n",
        "        \"---------------------\\n\"\n",
        "        \"Dada la información de contexto anterior, y sin utilizar conocimiento previo, responde la siguiente pregunta.\\n\"\n",
        "        \"Pregunta: {query_str}\\n\"\n",
        "        \"Respuesta: \"\n",
        "    )\n",
        "    return template.format(context_str=context_str, query_str=query_str)\n",
        "\n",
        "# Consultas y generación de respuestas\n",
        "queries = ['¿Cuál fue la primera película argentina?',\n",
        "           '¿Quién fue el director de la película \"La historia oficial\"?',\n",
        "           '¿Cuál es la película argentina más taquillera de todos los tiempos?',\n",
        "           '¿Cuántos premios Oscar ha ganado el cine argentino?',\n",
        "           '¿Cuál es la película argentina más premiada internacionalmente?']\n",
        "\n",
        "for query_str in queries:\n",
        "    results = embeddings.search(query_str, limit=2)\n",
        "\n",
        "    # Ajustar el acceso a los resultados basándose en la estructura\n",
        "    if isinstance(results, list) and len(results) > 0 and isinstance(results[0], dict):\n",
        "        context_str = \"\\n\".join([str(result.get(\"text\", \"\")) for result in results])\n",
        "    elif isinstance(results, list) and len(results) > 0 and isinstance(results[0], tuple):\n",
        "        context_str = \"\\n\".join([str(result[1]) for result in results])\n",
        "    else:\n",
        "        context_str = \"\"\n",
        "\n",
        "    final_prompt = prepare_prompt(query_str, context_str)\n",
        "    print('Pregunta:', query_str)\n",
        "    print('Respuesta:')\n",
        "    print(generate_answer(final_prompt))\n",
        "    print('-------------------------------------------------------')\n",
        "\n",
        "# Ejemplo de obtención de datos de una película de Wikidata\n",
        "def obtener_qid_pelicula(nombre_pelicula):\n",
        "    consulta_sparql = f\"\"\"\n",
        "    SELECT ?item WHERE {{\n",
        "      ?item rdfs:label \"{nombre_pelicula}\"@es.\n",
        "      ?item wdt:P31 wd:Q11424.\n",
        "    }}\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "    sparql.setQuery(consulta_sparql)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    try:\n",
        "        results = sparql.query().convert()\n",
        "        if results['results']['bindings']:\n",
        "            qid = results['results']['bindings'][0]['item']['value'].split('/')[-1]\n",
        "            return qid\n",
        "        else:\n",
        "            print(f\"No se encontraron resultados para la película '{nombre_pelicula}'.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error al obtener QID para la película '{nombre_pelicula}': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def obtener_descripcion(pelicula_qid):\n",
        "    property_ids = {\n",
        "        \"P1476\": \"Título original\",\n",
        "        \"P57\": \"Director\",\n",
        "        \"P161\": \"Elenco\",\n",
        "        \"P1040\": \"Productor\",\n",
        "        \"P344\": \"Director de fotografía\",\n",
        "        \"P58\": \"Guionista\",\n",
        "        \"P272\": \"Compañía productora\",\n",
        "        \"P577\": \"Fecha de lanzamiento\",\n",
        "        \"P495\": \"País de origen\",\n",
        "        \"P136\": \"Género\",\n",
        "        \"P915\": \"Locación de filmación\",\n",
        "        \"P162\": \"Productor ejecutivo\",\n",
        "        \"P144\": \"Basado en\",\n",
        "        \"P921\": \"Tema principal\",\n",
        "        \"P400\": \"Plataforma de distribución\",\n",
        "        \"P462\": \"Color\",\n",
        "        \"P2047\": \"Duración\",\n",
        "        \"P480\": \"Clasificación de contenido\",\n",
        "        \"P2638\": \"Presentación\"\n",
        "    }\n",
        "\n",
        "    description = f'Película {pelicula_qid}  :\\n'\n",
        "\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "\n",
        "    for propiedad_id, propiedad_name in property_ids.items():\n",
        "        consulta_sparql = f\"\"\"\n",
        "        SELECT ?valueLabel WHERE {{\n",
        "          wd:{pelicula_qid} wdt:{propiedad_id} ?value.\n",
        "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        sparql.setQuery(consulta_sparql)\n",
        "        sparql.setReturnFormat(JSON)\n",
        "\n",
        "        try:\n",
        "            results = sparql.query().convert()\n",
        "            time.sleep(1)  # Delay de 1 segundo entre solicitudes\n",
        "        except Exception as e:\n",
        "            print(f\"Error en la consulta SPARQL para la propiedad {propiedad_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "        valores = [result['valueLabel']['value'] for result in results['results']['bindings']]\n",
        "\n",
        "        if not valores:\n",
        "            print(f\"No se encontraron valores para la propiedad {propiedad_name} ({propiedad_id})\")\n",
        "        else:\n",
        "            print(f\"Valores encontrados para la propiedad {propiedad_name} ({propiedad_id}): {valores}\")\n",
        "\n",
        "        for valor in valores:\n",
        "            description += f'{propiedad_name}: {valor} \\n'\n",
        "\n",
        "    return description\n"
      ],
      "metadata": {
        "id": "LoMXdFe5hG3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 2: Preguntas**"
      ],
      "metadata": {
        "id": "HEBHet5ohhFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Explique con sus palabras el concepto de Rerank y como impactaría en el\n",
        "desempeño de la aplicación. Incluir un diagrama de elaboración propia en la\n",
        "explicación.\n",
        "\n",
        "Rerank se basa en reevaluar y reorganizar los documentos o datos recuperados en función de su relevancia para la consulta.\n",
        "Lo que genera esto es que se generen mejores resultados en base a la consulta. Mejora la calidad y precisión general de la información que se utiliza para la salida final.\n",
        "Por ejemplo: estamos en una plataforma de películas, dónde el sistema utiliza un modelo inicial para generar una lista de películas basándose en otros títulos que hemos visto.\n",
        "Luego la plataforma aplica un modelo de reranking para ajustar el orden de las recomendaciones, teniendo en cuenta otros aspectos, como pueden ser preferencias específicas (actores, directores, país de origen, etc), o películas con valoraciones altas o buenas críticas, o bien, algún tema en específico que hayamos visto recientemente.    \n",
        "Entonces finalmente, la plataforma va a mostrarnos una lista de películas ordenadas en base a nuestros intereses.\n",
        "\n",
        "2) ¿En qué sección de su código lo aplicaría?\n",
        "En este caso lo podría usar luego de la búsqueda en Chromadb, dónde busca por similitud de embeddings, el modelo genera una lista de los embeddings mas cercanos pero tal vez esa lista no es la óptima en el contexto. Ahí podría entrar rerank, para optimizar la consulta.\n"
      ],
      "metadata": {
        "id": "PbCFPikghuDD"
      }
    }
  ]
}